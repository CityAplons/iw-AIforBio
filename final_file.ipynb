{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from numpy import linalg as LA\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_mutation(line):\n",
    "    sequence,current,position,mutation,ddG,PH,Temp = line\n",
    "    current,mutation = mutation,current\n",
    "    sequence = sequence[:int(position)]+current+sequence[int(position)+1:]\n",
    "    ddG = str(-float(ddG))\n",
    "    return [sequence,current,position,mutation,ddG,PH,Temp]\n",
    "def read_data(file_name):\n",
    "    conn = sql.connect(file_name)\n",
    "    cursor = conn.cursor()\n",
    "    types = [str,str,int,str,float,float,float]\n",
    "    names = [\"sequence\",\"current\",\"position\",\"mutation\",\"ddG\",\"PH\",\"Temp\"]\n",
    "    results = {names[i]:[] for i in range(len(names))}\n",
    "    for row in cursor.execute(\"SELECT * FROM dataset\"):\n",
    "        row = list(row)\n",
    "        row[2] = str(int(row[2])-1)\n",
    "        inversed = inverse_mutation(row)\n",
    "        for i in range(len(names)):\n",
    "            results[names[i]].append(types[i](row[i]))\n",
    "            #results[names[i]].append(types[i](inversed[i]))\n",
    "    result = pd.DataFrame(results)\n",
    "    conn.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampling(data_set):\n",
    "    number_plus = len(data_set[data_set.ddG > 0])\n",
    "    number_minus = len(data_set[data_set.ddG < 0])\n",
    "    multiply = 0\n",
    "    plus = 0\n",
    "    new_number_plus = number_plus\n",
    "    new_number_minus = number_minus\n",
    "    while new_number_plus+number_plus < new_number_minus:\n",
    "        new_number_plus += number_plus\n",
    "        multiply += 1\n",
    "    plus = new_number_minus - new_number_plus\n",
    "    #need to do\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbour(sequence,position,shift):\n",
    "    if (position + shift > len(sequence)-1 or position + shift < 0):\n",
    "        return \"0\"\n",
    "    else:\n",
    "        return sequence[position+shift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_true_false(label, true_label):\n",
    "    count_true = 0\n",
    "    count_all = 0\n",
    "    for i in range(len(label)):\n",
    "        count_all += 1\n",
    "        if (np.sign(true_label[i]) == np.sign(label[i])):\n",
    "            count_true += 1\n",
    "    return float(count_true)/count_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_symmetry(estimator,test_data,test_labels):\n",
    "    columns = map(str,test_data.columns)\n",
    "    inversed_test_data = test_data.copy()\n",
    "    inversed_test_data.current,inversed_test_data.mutation = inversed_test_data.mutation.apply(lambda x: x),inversed_test_data.current.apply(lambda x: x)\n",
    "    inversed_labels = estimator.predict(inversed_test_data)\n",
    "    test_labels = np.array(test_labels)\n",
    "    inversed_labels = np.array(inversed_labels)\n",
    "    vector = test_labels+inversed_labels\n",
    "    return LA.norm(vector)/len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate():\n",
    "    #read dataset\n",
    "    raw_data = read_data(\"iwdb.sqlite\")\n",
    "    #edit new features\n",
    "    number_neighbours = 3\n",
    "    for i in range(1,number_neighbours+1):\n",
    "        raw_data['Left'+str(i)] = raw_data.apply(lambda row: neighbour(row['sequence'],row['position'],-(i)),axis=1)\n",
    "        raw_data['Right'+str(i)] = raw_data.apply(lambda row: neighbour(row['sequence'],row['position'],(i)),axis=1)\n",
    "    #shuffle data\n",
    "    raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "    #change symbols in features to int values\n",
    "    inputting = 'A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0'\n",
    "    symbols = inputting.split(' ')\n",
    "    numerics = [i for i in range(len(symbols))]\n",
    "    categorical = ['current','mutation']+['Left'+str(i) for i in range(1,number_neighbours+1)]+['Right'+str(i) for i in range(1,number_neighbours+1)]\n",
    "    for item in categorical:\n",
    "        raw_data[item] = raw_data[item].apply(lambda x: numerics[symbols.index(x)])\n",
    "    #make train and test data\n",
    "    train_size = 0.8\n",
    "    size = int(len(raw_data)*train_size)\n",
    "    train_data = raw_data.iloc[:size]\n",
    "    test_data = raw_data.iloc[size:]\n",
    "    train_labels = train_data['ddG'].values\n",
    "    train_data = train_data.drop(['ddG','sequence'],axis=1)\n",
    "    test_labels = test_data['ddG'].values\n",
    "    test_data = test_data.drop(['ddG','sequence'],axis=1)\n",
    "    #make foundation for pipeline\n",
    "    binary_data_columns = ['holiday', 'workingday']\n",
    "    binary_data_indices = np.array([(column in binary_data_columns) for column in train_data.columns], dtype = bool)\n",
    "    categorical_data_indices = np.array([(column in categorical) for column in train_data.columns], dtype = bool)\n",
    "    numeric_data_columns = ['Temp', 'PH', 'position']\n",
    "    numeric_data_indices = np.array([(column in numeric_data_columns) for column in train_data.columns], dtype = bool)\n",
    "    #creat regressor\n",
    "    dtrain = xgb.DMatrix(train_data.values,train_labels)\n",
    "    dtest = xgb.DMatrix(test_data.values)\n",
    "    param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "    num_round = 2\n",
    "    regr = RandomForestRegressor(random_state = 0, max_depth = 20, n_estimators = 100)\n",
    "    #regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    estimator = pipeline.Pipeline(steps = [       \n",
    "        ('feature_processing', pipeline.FeatureUnion(transformer_list = [        \n",
    "                #binary\n",
    "                ('binary_variables_processing', preprocessing.FunctionTransformer(lambda data: data[:, binary_data_indices])), \n",
    "                    \n",
    "                #numeric\n",
    "                ('numeric_variables_processing', pipeline.Pipeline(steps = [\n",
    "                     ('selecting', preprocessing.FunctionTransformer(lambda data: data[:, numeric_data_indices]))\n",
    "                            ])),\n",
    "        \n",
    "                #categorical\n",
    "                ('categorical_variables_processing', pipeline.Pipeline(steps = [\n",
    "                    ('selecting', preprocessing.FunctionTransformer(lambda data: data[:, categorical_data_indices]))            \n",
    "                            ])),\n",
    "            ])),\n",
    "        ('model_fitting', regr)\n",
    "        ]\n",
    "    )\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    # make prediction\n",
    "    predictions = bst.predict(dtest)\n",
    "    #estimator.fit(train_data,train_labels)\n",
    "    #metrics.mean_absolute_error(test_labels, estimator.predict(test_data))\n",
    "    #test_true_false(estimator.predict(test_data),test_labels)\n",
    "    #test_symmetry(test_data,test_labels)\n",
    "    #predictions = estimator.predict(test_data)\n",
    "    #with open('my_dumped_classifier.pkl', 'wb') as fid:\n",
    "        #cPickle.dump(regr, fid)    \n",
    "    #print(metrics.mean_squared_error(test_labels,predictions),test_true_false(predictions,test_labels),test_symmetry(estimator,test_data,test_labels))\n",
    "    print(metrics.mean_squared_error(test_labels,predictions),test_true_false(predictions,test_labels),test_symmetry(bst,test_data,test_labels))\n",
    "    #test_load(test_data,test_labels,binary_data_indices,numeric_data_indices,categorical_data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.4095932727289064, 0.8148984198645598, 0.1208164103383151)\n",
      "(1.4095932727289064, 0.8148984198645598, 0.1208164103383151)\n"
     ]
    }
   ],
   "source": [
    "calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it again\n",
    "def test_load(test_data,test_labels,binary_data_indices,numeric_data_indices,categorical_data_indices):\n",
    "    with open('my_dumped_classifier.pkl', 'rb') as fid:\n",
    "        gnb_loaded = cPickle.load(fid)\n",
    "    estimator = pipeline.Pipeline(steps = [       \n",
    "            ('feature_processing', pipeline.FeatureUnion(transformer_list = [        \n",
    "                    #binary\n",
    "                    ('binary_variables_processing', preprocessing.FunctionTransformer(lambda data: data[:, binary_data_indices])), \n",
    "                    \n",
    "                    #numeric\n",
    "                    ('numeric_variables_processing', pipeline.Pipeline(steps = [\n",
    "                         ('selecting', preprocessing.FunctionTransformer(lambda data: data[:, numeric_data_indices]))\n",
    "                                ])),\n",
    "        \n",
    "                    #categorical\n",
    "                    ('categorical_variables_processing', pipeline.Pipeline(steps = [\n",
    "                        ('selecting', preprocessing.FunctionTransformer(lambda data: data[:, categorical_data_indices]))            \n",
    "                                ])),\n",
    "                ])),\n",
    "            ('model_fitting', gnb_loaded)\n",
    "            ]\n",
    "        )\n",
    "    predictions = estimator.predict(test_data)\n",
    "    print(metrics.mean_squared_error(test_labels,predictions),test_true_false(predictions,test_labels),test_symmetry(estimator,test_data,test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
